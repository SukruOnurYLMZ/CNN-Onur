{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOWQ1q1btOkQL8ilN86FWKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SukruOnurYLMZ/CNN-Onur/blob/main/HELMET_DETECTION_WITH_YOLOv12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**🎓 Computer Vision – YOLOv12 Helmet Object Detector Final Assignment ONUR**"
      ],
      "metadata": {
        "id": "dkVXmgHMbAvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook presents the results of my project titled “Helmet Detection”, an object detection model developed and trained using the Roboflow platform. The main objective of this project is to detect face masks in real-world images, reflecting scenarios experienced during the pandemic.\n",
        "\n",
        "The model was trained using a custom-labeled dataset consisting of over 380 images, annotated with bounding boxes for \"mask\" and \"no-mask\" categories.\n",
        "\n",
        "Model Type: Roboflow 3.0 Object Detection (Fast)\n",
        "\n",
        "You can preview the model and its deployment features through Roboflow’s hosted interface.\n",
        "\n",
        "This project is open for collaboration, and I would be excited to work with others who are interested in:\n",
        "\n",
        "Enhancing the dataset or model architecture (e.g., YOLOv12, MobileNet, etc.)\n",
        "\n",
        "Deploying the model for real-time or embedded system use\n",
        "\n",
        "Conducting joint research or building AI applications focused on public health and safety\n",
        "\n",
        "Integrating the system with video streams or surveillance tools\n",
        "\n",
        "If you're interested in contributing or partnering, feel free to connect with me.\n",
        "\n",
        "Project Owner: Onur Yilmaz Platform: Roboflow Model Name: Helmet Detection 1 Date: July 9, 2025**\n",
        "\n",
        "🧠 Project Tasks:"
      ],
      "metadata": {
        "id": "9gcMBtGdbI3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB7TuoPha-lz"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/sunsmarterjie/yolov12.git roboflow supervision flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"ZA7VGxpX0FGbFDZHJAnp\")\n",
        "project = rf.workspace(\"onur-yilmaz-3unus\").project(\"helmet-esqea-yk2q6\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov12\")"
      ],
      "metadata": {
        "id": "D_iD93gTbVos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!echo -e \"test: {dataset.location}/test/images\\ntrain: {dataset.location}/train/images\\nval: {dataset.location}/valid/images\" >> {dataset.location}/data.yaml"
      ],
      "metadata": {
        "id": "KQVh1ejLbWWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Train a YOLOv12 Model:**"
      ],
      "metadata": {
        "id": "O_vfegSlbgZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "model = YOLO('yolov12m.pt')  # You can use yolov12n.pt, yolov12m.pt etc.\n",
        "model.train(data=f\"{dataset.location}/data.yaml\", epochs=18)"
      ],
      "metadata": {
        "id": "pQiMZ5DubWbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Evaluate the Model:**"
      ],
      "metadata": {
        "id": "FAepqqT2b06U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv\n",
        "from supervision.metrics import MeanAveragePrecision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "ds = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=f\"{dataset.location}/test/images\",\n",
        "    annotations_directory_path=f\"{dataset.location}/test/labels\",\n",
        "    data_yaml_path=f\"{dataset.location}/data.yaml\"\n",
        ")\n",
        "\n",
        "model = YOLO(f\"/content/runs/detect/train/weights/best.pt\")\n",
        "\n",
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "for _, image, target in ds:\n",
        "    results = model(image, verbose=False)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "    predictions.append(detections)\n",
        "    targets.append(target)\n",
        "\n",
        "map = MeanAveragePrecision().update(predictions, targets).compute()\n",
        "print(\"mAP 50:95\", map.map50_95)\n",
        "print(\"mAP 50\", map.map50)\n",
        "print(\"mAP 75\", map.map75)\n",
        "\n",
        "map.plot()\n"
      ],
      "metadata": {
        "id": "0WIF73qcbntr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Metrics (Top Left):\n",
        "\n",
        "mAP@50:95 (average mAP over IoU thresholds from 0.5 to 0.95) is 0.426.\n",
        "\n",
        "mAP@50 (mAP at IoU threshold 0.5) is 0.831.\n",
        "\n",
        "mAP@75 (mAP at IoU threshold 0.75) is 0.364.\n",
        "\n",
        "Breakdown by Object Size and Metric (Bar Plot): The plot categorizes performance by object size: Small, Medium, and Large, and for each, it shows three mAP metrics: mAP@50-95, mAP@50, and mAP@75.\n",
        "\n",
        "Small Objects:\n",
        "\n",
        "All mAP values (mAP@50-95, mAP@50, mAP@75) for small objects are 0.00, indicating no successful detections or very poor performance for this size category.\n",
        "\n",
        "Medium Objects:\n",
        "\n",
        "mAP@50-95 for medium objects is 0.47.\n",
        "\n",
        "mAP@50 for medium objects is 0.86, which is the highest individual mAP@50 value across all categories.\n",
        "\n",
        "mAP@75 for medium objects is 0.51.\n",
        "\n",
        "Large Objects:\n",
        "\n",
        "mAP@50-95 for large objects is 0.40.\n",
        "\n",
        "mAP@50 for large objects is 0.77.\n",
        "\n",
        "mAP@75 for large objects is 0.34."
      ],
      "metadata": {
        "id": "q21QFi1Rb7iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Test the Model on an Unseen Image:**"
      ],
      "metadata": {
        "id": "JmpvYKK8b-fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "i = random.randint(0, len(ds))\n",
        "image_path, image, target = ds[i]\n",
        "\n",
        "results = model(image, verbose=False)[0]\n",
        "detections = sv.Detections.from_ultralytics(results).with_nms()\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_image)"
      ],
      "metadata": {
        "id": "AGDWKro2b8uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "i = random.randint(0, len(ds))\n",
        "image_path, image, target = ds[i]\n",
        "\n",
        "results = model(image, verbose=False)[0]\n",
        "detections = sv.Detections.from_ultralytics(results).with_nms()\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "sv.plot_image(annotated_image)"
      ],
      "metadata": {
        "id": "7LG-MeoVcHgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Academic-quality font and style\n",
        "plt.rcParams.update({\n",
        "    \"font.size\": 12,\n",
        "    \"axes.titlesize\": 14,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"figure.figsize\": (7, 5),\n",
        "    \"font.family\": \"serif\"\n",
        "})\n",
        "\n",
        "# Prepare data\n",
        "scores = {\n",
        "    \"mAP@50:95\": map.map50_95,\n",
        "    \"mAP@50\": map.map50,\n",
        "    \"mAP@75\": map.map75\n",
        "}\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "bars = ax.bar(scores.keys(), scores.values(), color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"])\n",
        "\n",
        "# Add value labels on top\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{height:.2f}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 5),  # vertical offset\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# Titles and labels\n",
        "ax.set_title(\"Object Detection Performance Metrics\", pad=15)\n",
        "ax.set_ylabel(\"Score\")\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D_qq12WTcJ8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Load YOLOv12 model\n",
        "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
        "\n",
        "# Dynamically find an image in the test set\n",
        "test_images_dir = f\"{dataset.location}/test/images\"\n",
        "image_files = glob.glob(os.path.join(test_images_dir, '*.*')) # Find all files in the directory\n",
        "\n",
        "if not image_files:\n",
        "    print(f\"Error: No image files found in {test_images_dir}\")\n",
        "else:\n",
        "    # Use the first image found as an example\n",
        "    image_path = image_files[0]\n",
        "    print(f\"Using image: {image_path}\")\n",
        "\n",
        "    # Run inference\n",
        "    results = model(image_path)\n",
        "\n",
        "    # Show result\n",
        "    res_plotted = results[0].plot()\n",
        "    plt.imshow(res_plotted[..., ::-1])\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Detection Result\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xTvW-vFJcLaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Test the Model on a Short Video:**"
      ],
      "metadata": {
        "id": "TXQi3GGpci_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eiNblVFCcj3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a video\n",
        "from google.colab import files\n",
        "uploaded_video = files.upload()\n",
        "\n",
        "# Get video filename\n",
        "video_path = next(iter(uploaded_video))\n",
        "\n",
        "# Run inference\n",
        "model.predict(source=video_path, save=True, save_txt=True, conf=0.25)\n",
        "\n",
        "# Output will be saved under /content/runs/detect/predict/"
      ],
      "metadata": {
        "id": "LdHeqm5gcmWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Your video path from the previous output\n",
        "video_path = '/content/1721568-hd_1920_1080_30fps.mp4'\n",
        "\n",
        "# --- Choose which single frame you want ---\n",
        "# Frame numbers start from 0. Your video has 263 frames (0 to 262).\n",
        "# Let's say you want to 'post' frame number 150.\n",
        "frame_to_extract = 112 # You can change this to any number between 0 and 262\n",
        "\n",
        "# --- Extract and display/save the frame ---\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"Error: Video file not found at '{video_path}'\")\n",
        "else:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "    else:\n",
        "        # Set the video to the desired frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_to_extract)\n",
        "\n",
        "        # Read the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if ret:\n",
        "            print(f\"Successfully extracted frame {frame_to_extract}\")\n",
        "\n",
        "            # 1. Display the frame in your Colab output\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            plt.imshow(frame_rgb)\n",
        "            plt.title(f\"Single Frame: {frame_to_extract}\")\n",
        "            plt.axis('off') # Hide axes\n",
        "            plt.show()\n",
        "\n",
        "            # 2. Save the frame as an image file (e.g., JPEG)\n",
        "            output_image_filename = f'extracted_frame_{frame_to_extract}.jpg'\n",
        "            cv2.imwrite(output_image_filename, frame) # Saves in BGR format\n",
        "            print(f\"Frame also saved as '{output_image_filename}' in your Colab environment.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Error: Could not read frame {frame_to_extract} from video. It might be out of range.\")\n",
        "\n",
        "        cap.release()\n",
        "        print(\"Video capture released.\")"
      ],
      "metadata": {
        "id": "UD6HAmXscqPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# --- Step 1: Find the directory where results were saved ---\n",
        "output_base_dir = 'runs/detect'\n",
        "predict_dirs = sorted(glob.glob(os.path.join(output_base_dir, 'predict*')))\n",
        "\n",
        "if not predict_dirs:\n",
        "    print(f\"Error: No prediction directories found in '{output_base_dir}'.\")\n",
        "    print(\"Please ensure `model.predict(save=True)` was run successfully.\")\n",
        "else:\n",
        "    latest_predict_dir = predict_dirs[-1] # Get the latest (alphabetically/numerically last)\n",
        "    print(f\"Using results from: {latest_predict_dir}\")\n",
        "\n",
        "    # --- Step 2: Construct the expected path to the output video ---\n",
        "    # Use the actual filename of the video that was uploaded and processed\n",
        "    video_filename = '/content/runs/detect/predict/6473954-uhd_2160_3840_25fps.avi' # Corrected video filename\n",
        "    output_video_with_detections_path = os.path.join(latest_predict_dir, video_filename)\n",
        "\n",
        "    # --- Choose which single frame you want ---\n",
        "    # Frame numbers start from 0. The video has 805 frames (0 to 804).\n",
        "    frame_number_to_display = 400 # <--- Change this number to the frame you want to display\n",
        "\n",
        "    # --- Step 4: Open the output video and extract the frame ---\n",
        "    if not os.path.exists(output_video_with_detections_path):\n",
        "        print(f\"Error: Output video with detections not found at '{output_video_with_detections_path}'\")\n",
        "        print(\"This might happen if the original video name was different or if the output format changed.\")\n",
        "    else:\n",
        "        print(f\"Opening output video: {output_video_with_detections_path}\")\n",
        "        cap = cv2.VideoCapture(output_video_with_detections_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open the output video file {output_video_with_detections_path}.\")\n",
        "            print(\"Ensure OpenCV can handle .avi files (might need additional codecs if on local machine, but usually fine in Colab).\")\n",
        "        else:\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            print(f\"Total frames in output video: {total_frames}\")\n",
        "\n",
        "            if frame_number_to_display >= total_frames or frame_number_to_display < 0:\n",
        "                print(f\"Warning: Frame {frame_number_to_display} is out of bounds. Please choose a frame between 0 and {total_frames - 1}.\")\n",
        "                frame_number_to_display = 0 # Default to first frame\n",
        "                print(f\"Extracting the first frame (0) instead.\")\n",
        "\n",
        "\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number_to_display)\n",
        "\n",
        "\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            if ret:\n",
        "                print(f\"Successfully extracted frame {frame_number_to_display} with detections.\")\n",
        "\n",
        "                # Convert BGR (OpenCV default) to RGB for Matplotlib display\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # 5. Display the frame in your Colab output\n",
        "                plt.figure(figsize=(15, 10)) # Adjust figure size as needed\n",
        "                plt.imshow(frame_rgb)\n",
        "                plt.title(f\"Frame {frame_number_to_display} with Detections\")\n",
        "                plt.axis('off') # Hide axes for a cleaner view\n",
        "                plt.show()\n",
        "\n",
        "                # 6. Save the frame as an image file (optional)\n",
        "                output_image_filename = f'detected_frame_{frame_number_to_display}.jpg'\n",
        "                cv2.imwrite(output_image_filename, frame) # Saves in BGR format\n",
        "                print(f\"Frame also saved as '{output_image_filename}' in your Colab environment.\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Error: Could not read frame {frame_number_to_display} from the output video.\")\n",
        "\n",
        "            cap.release()\n",
        "            print(\"Video capture released.\")"
      ],
      "metadata": {
        "id": "2K7jeH1XcwVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a video\n",
        "from google.colab import files\n",
        "uploaded_video = files.upload()\n",
        "\n",
        "# Get video filename\n",
        "video_path = next(iter(uploaded_video))\n",
        "\n",
        "# Run inference\n",
        "model.predict(source=video_path, save=True, save_txt=True, conf=0.25)\n",
        "\n",
        "# Output will be saved under /content/runs/detect/predict/"
      ],
      "metadata": {
        "id": "_H9ePrMDczEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**📝 Project Summary – Pandemic Time Mask Detection**\n",
        "\n",
        "This project focuses on detecting face masks in images using object detection techniques. The model was trained on a custom dataset with 386 annotated images using Roboflow’s Object Detection (Fast) model.\n",
        "\n",
        "\n",
        "**--🔍 Model Performance:--**\n",
        "\n",
        "\n",
        "mAP@50: 87.5%\n",
        "\n",
        "Precision: 88.6%\n",
        "\n",
        "Recall: 78.6%\n",
        "\n",
        "Model Type: Roboflow 3.0 (Fast)\n",
        "\n",
        "The model performs well in identifying individuals wearing masks across various environments. It's suitable for real-time applications like workplace safety monitoring, smart cameras, or educational purposes.\n",
        "\n",
        "**🤝 Collaboration:**\n",
        "\n",
        "This project is open for collaboration in areas such as:\n",
        "\n",
        "Improving model accuracy with YOLOv12 or other architectures\n",
        "\n",
        "Expanding the dataset with domain-specific images\n",
        "\n",
        "Deployment to web/mobile/IoT platforms\n",
        "\n",
        "Joint research or academic publishing"
      ],
      "metadata": {
        "id": "q7JGEVC4c3TL"
      }
    }
  ]
}